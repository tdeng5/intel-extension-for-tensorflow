diff --git a/llvm/include/llvm/InitializePasses.h b/llvm/include/llvm/InitializePasses.h
index c123d483caf7..e6e0d57d562d 100644
--- a/llvm/include/llvm/InitializePasses.h
+++ b/llvm/include/llvm/InitializePasses.h
@@ -107,6 +107,7 @@ void initializeCrossDSOCFIPass(PassRegistry&);
 void initializeCycleInfoWrapperPassPass(PassRegistry &);
 void initializeDAEPass(PassRegistry&);
 void initializeDAHPass(PassRegistry&);
+void initializeDAESYCLPass(PassRegistry&);
 void initializeDCELegacyPassPass(PassRegistry&);
 void initializeDFAJumpThreadingLegacyPassPass(PassRegistry &);
 void initializeDSELegacyPassPass(PassRegistry&);
@@ -404,6 +405,9 @@ void initializeStripNonDebugSymbolsPass(PassRegistry&);
 void initializeStripNonLineTableDebugLegacyPassPass(PassRegistry &);
 void initializeStripSymbolsPass(PassRegistry&);
 void initializeStructurizeCFGLegacyPassPass(PassRegistry &);
+void initializeSYCLLowerWGScopeLegacyPassPass(PassRegistry &);
+void initializeSYCLMutatePrintfAddrspaceLegacyPassPass(PassRegistry &);
+void initializeSYCLLowerWGLocalMemoryLegacyPass(PassRegistry &);
 void initializeTailCallElimPass(PassRegistry&);
 void initializeTailDuplicatePass(PassRegistry&);
 void initializeTargetLibraryInfoWrapperPassPass(PassRegistry&);
diff --git a/llvm/include/llvm/SYCLLowerIR/LocalAccessorToSharedMemory.h b/llvm/include/llvm/SYCLLowerIR/LocalAccessorToSharedMemory.h
new file mode 100644
index 000000000000..9dfd6dc18926
--- /dev/null
+++ b/llvm/include/llvm/SYCLLowerIR/LocalAccessorToSharedMemory.h
@@ -0,0 +1,69 @@
+//===- LocalAccessorToSharedMemory.cpp - Local Accessor Support for CUDA --===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_SYCL_LOCALACCESSORTOSHAREDMEMORY_H
+#define LLVM_SYCL_LOCALACCESSORTOSHAREDMEMORY_H
+
+#include "llvm/IR/Module.h"
+#include "llvm/IR/PassManager.h"
+#include "llvm/SYCLLowerIR/TargetHelpers.h"
+
+namespace llvm {
+
+class ModulePass;
+class PassRegistry;
+
+/// This pass operates on SYCL kernels. It modifies kernel entry points which
+/// take pointers to shared memory and alters them to take offsets into shared
+/// memory (represented by a symbol in the shared address space). The SYCL
+/// runtime is expected to provide offsets rather than pointers to these
+/// functions.
+class LocalAccessorToSharedMemoryPass
+    : public PassInfoMixin<LocalAccessorToSharedMemoryPass> {
+private:
+  using KernelPayload = TargetHelpers::KernelPayload;
+  using ArchType = TargetHelpers::ArchType;
+
+public:
+  explicit LocalAccessorToSharedMemoryPass() {}
+
+  PreservedAnalyses run(Module &M, ModuleAnalysisManager &);
+  static StringRef getPassName() {
+    return "SYCL Local Accessor to Shared Memory";
+  }
+
+private:
+  /// This function replaces pointers to shared memory with offsets to a global
+  /// symbol in shared memory.
+  /// It alters the signature of the kernel (pointer vs offset value) as well
+  /// as the access (dereferencing the argument pointer vs GEP to the global
+  /// symbol).
+  ///
+  /// \param F The kernel to be processed.
+  ///
+  /// \returns A new function with global symbol accesses.
+  Function *processKernel(Module &M, Function *F);
+
+  /// Update kernel metadata to reflect the change in the signature.
+  ///
+  /// \param A map of original kernels to the modified ones.
+  void postProcessKernels(
+      SmallVectorImpl<std::pair<Function *, KernelPayload>> &NewToOldKernels);
+
+private:
+  /// The value for NVVM's ADDRESS_SPACE_SHARED and AMD's LOCAL_ADDRESS happen
+  /// to be 3.
+  const unsigned SharedASValue = 3;
+};
+
+ModulePass *createLocalAccessorToSharedMemoryPassLegacy();
+void initializeLocalAccessorToSharedMemoryLegacyPass(PassRegistry &);
+
+} // end namespace llvm
+
+#endif
diff --git a/llvm/include/llvm/SYCLLowerIR/LowerWGLocalMemory.h b/llvm/include/llvm/SYCLLowerIR/LowerWGLocalMemory.h
new file mode 100644
index 000000000000..cdb8e6dff021
--- /dev/null
+++ b/llvm/include/llvm/SYCLLowerIR/LowerWGLocalMemory.h
@@ -0,0 +1,52 @@
+//===-- LowerWGLocalMemory.h - SYCL kernel local memory allocation pass ---===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+// This pass does the following for each allocate call to
+// __sycl_allocateLocalMemory(Size, Alignment) function at the kernel scope:
+// - inserts a global (in scope of a program) byte array of Size bytes with
+//   specified alignment in work group local address space.
+// - replaces allocate call with access to this memory.
+//
+// For example, the following IR code in a kernel function:
+//   define spir_kernel void @KernelA() {
+//     %0 = call spir_func i8 addrspace(3)* @__sycl_allocateLocalMemory(
+//         i64 128, i64 4)
+//     %1 = bitcast i8 addrspace(3)* %0 to i32 addrspace(3)*
+//   }
+//
+// is translated to the following:
+//   @WGLocalMem = internal addrspace(3) global [128 x i8] undef, align 4
+//   define spir_kernel void @KernelA() {
+//     %0 = bitcast i8 addrspace(3)* getelementptr inbounds (
+//         [128 x i8], [128 x i8] addrspace(3)* @WGLocalMem, i32 0, i32 0)
+//         to i32 addrspace(3)*
+//   }
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_SYCLLOWERIR_LOWERWGLOCALMEMORY_H
+#define LLVM_SYCLLOWERIR_LOWERWGLOCALMEMORY_H
+
+#include "llvm/IR/Module.h"
+#include "llvm/IR/PassManager.h"
+
+namespace llvm {
+
+class ModulePass;
+class PassRegistry;
+
+class SYCLLowerWGLocalMemoryPass
+    : public PassInfoMixin<SYCLLowerWGLocalMemoryPass> {
+public:
+  PreservedAnalyses run(Module &M, ModuleAnalysisManager &);
+};
+
+ModulePass *createSYCLLowerWGLocalMemoryLegacyPass();
+void initializeSYCLLowerWGLocalMemoryLegacyPass(PassRegistry &);
+
+} // namespace llvm
+
+#endif // LLVM_SYCLLOWERIR_LOWERWGLOCALMEMORY_H
diff --git a/llvm/include/llvm/SYCLLowerIR/LowerWGScope.h b/llvm/include/llvm/SYCLLowerIR/LowerWGScope.h
new file mode 100644
index 000000000000..454159249edf
--- /dev/null
+++ b/llvm/include/llvm/SYCLLowerIR/LowerWGScope.h
@@ -0,0 +1,33 @@
+//===-- LowerWGScope.h - lower work group-scope code ----------------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef CLANG_LIB_CODEGEN_SYCLLOWERIR_LOWERWGCODE_H
+#define CLANG_LIB_CODEGEN_SYCLLOWERIR_LOWERWGCODE_H
+
+#include "llvm/IR/Function.h"
+#include "llvm/IR/PassManager.h"
+
+namespace llvm {
+
+class FunctionPass;
+
+/// SPIRV target specific pass to transform work group-scope code to match SIMT
+/// execution model semantics - this code must be executed once per work group.
+class SYCLLowerWGScopePass : public PassInfoMixin<SYCLLowerWGScopePass> {
+public:
+  PreservedAnalyses run(Function &F, FunctionAnalysisManager &);
+};
+
+FunctionPass *createSYCLLowerWGScopePass();
+
+} // namespace llvm
+
+#endif // CLANG_LIB_CODEGEN_SYCLLOWERIR_LOWERWGCODE_H
diff --git a/llvm/include/llvm/SYCLLowerIR/TargetHelpers.h b/llvm/include/llvm/SYCLLowerIR/TargetHelpers.h
new file mode 100644
index 000000000000..b2b383237a70
--- /dev/null
+++ b/llvm/include/llvm/SYCLLowerIR/TargetHelpers.h
@@ -0,0 +1,43 @@
+//===------------ TargetHelpers.h - Helpers for SYCL kernels ------------- ===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// Helper functions for processing SYCL kernels.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_SYCL_SYCL_LOWER_IR_TARGET_HELPERS_H
+#define LLVM_SYCL_SYCL_LOWER_IR_TARGET_HELPERS_H
+
+#include "llvm/ADT/SmallVector.h"
+#include "llvm/IR/Function.h"
+#include "llvm/IR/Module.h"
+
+using namespace llvm;
+
+namespace llvm {
+namespace TargetHelpers {
+
+enum class ArchType { Cuda, AMDHSA, Unsupported };
+
+struct KernelPayload {
+  KernelPayload(Function *Kernel, MDNode *MD = nullptr);
+  Function *Kernel;
+  MDNode *MD;
+};
+
+ArchType getArchType(const Module &M);
+
+std::string getAnnotationString(ArchType AT);
+
+void populateKernels(Module &M, SmallVectorImpl<KernelPayload> &Kernels,
+                     TargetHelpers::ArchType AT);
+
+} // end namespace TargetHelpers
+} // end namespace llvm
+
+#endif
diff --git a/llvm/lib/CMakeLists.txt b/llvm/lib/CMakeLists.txt
index 52772fefdb20..703b8ed14bfc 100644
--- a/llvm/lib/CMakeLists.txt
+++ b/llvm/lib/CMakeLists.txt
@@ -36,6 +36,7 @@ add_subdirectory(AsmParser)
 add_subdirectory(LineEditor)
 add_subdirectory(ProfileData)
 add_subdirectory(Passes)
+add_subdirectory(SYCLLowerIR)
 add_subdirectory(TextAPI)
 add_subdirectory(ToolDrivers)
 add_subdirectory(XRay)
diff --git a/llvm/lib/Passes/PassBuilderPipelines.cpp b/llvm/lib/Passes/PassBuilderPipelines.cpp
index df46119d9124..ff149b5a32d6 100644
--- a/llvm/lib/Passes/PassBuilderPipelines.cpp
+++ b/llvm/lib/Passes/PassBuilderPipelines.cpp
@@ -190,6 +190,10 @@ static cl::opt<bool> EnableGlobalAnalyses(
     "enable-global-analyses", cl::init(true), cl::Hidden,
     cl::desc("Enable inter-procedural analyses"));
 
+static cl::opt<bool>
+    SYCLOptimizationMode("sycl-opt", cl::init(false), cl::Hidden,
+                         cl::desc("Enable SYCL optimization mode."));
+
 static cl::opt<bool>
     RunPartialInlining("enable-partial-inlining", cl::init(false), cl::Hidden,
                        cl::desc("Run Partial inlinining pass"));
@@ -348,83 +352,87 @@ PassBuilder::buildO1FunctionSimplificationPipeline(OptimizationLevel Level,
   // Form canonically associated expression trees, and simplify the trees using
   // basic mathematical properties. For example, this will form (nearly)
   // minimal multiplication trees.
-  FPM.addPass(ReassociatePass());
-
-  // Add the primary loop simplification pipeline.
-  // FIXME: Currently this is split into two loop pass pipelines because we run
-  // some function passes in between them. These can and should be removed
-  // and/or replaced by scheduling the loop pass equivalents in the correct
-  // positions. But those equivalent passes aren't powerful enough yet.
-  // Specifically, `SimplifyCFGPass` and `InstCombinePass` are currently still
-  // used. We have `LoopSimplifyCFGPass` which isn't yet powerful enough yet to
-  // fully replace `SimplifyCFGPass`, and the closest to the other we have is
-  // `LoopInstSimplify`.
-  LoopPassManager LPM1, LPM2;
-
-  // Simplify the loop body. We do this initially to clean up after other loop
-  // passes run, either when iterating on a loop or on inner loops with
-  // implications on the outer loop.
-  LPM1.addPass(LoopInstSimplifyPass());
-  LPM1.addPass(LoopSimplifyCFGPass());
-
-  // Try to remove as much code from the loop header as possible,
-  // to reduce amount of IR that will have to be duplicated. However,
-  // do not perform speculative hoisting the first time as LICM
-  // will destroy metadata that may not need to be destroyed if run
-  // after loop rotation.
-  // TODO: Investigate promotion cap for O1.
-  LPM1.addPass(LICMPass(PTO.LicmMssaOptCap, PTO.LicmMssaNoAccForPromotionCap,
-                        /*AllowSpeculation=*/false));
-
-  LPM1.addPass(LoopRotatePass(/* Disable header duplication */ true,
-                              isLTOPreLink(Phase)));
-  // TODO: Investigate promotion cap for O1.
-  LPM1.addPass(LICMPass(PTO.LicmMssaOptCap, PTO.LicmMssaNoAccForPromotionCap,
-                        /*AllowSpeculation=*/true));
-  LPM1.addPass(SimpleLoopUnswitchPass());
-  if (EnableLoopFlatten)
-    LPM1.addPass(LoopFlattenPass());
-
-  LPM2.addPass(LoopIdiomRecognizePass());
-  LPM2.addPass(IndVarSimplifyPass());
-
-  for (auto &C : LateLoopOptimizationsEPCallbacks)
-    C(LPM2, Level);
-
-  LPM2.addPass(LoopDeletionPass());
-
-  if (EnableLoopInterchange)
-    LPM2.addPass(LoopInterchangePass());
-
-  // Do not enable unrolling in PreLinkThinLTO phase during sample PGO
-  // because it changes IR to makes profile annotation in back compile
-  // inaccurate. The normal unroller doesn't pay attention to forced full unroll
-  // attributes so we need to make sure and allow the full unroll pass to pay
-  // attention to it.
-  if (Phase != ThinOrFullLTOPhase::ThinLTOPreLink || !PGOOpt ||
-      PGOOpt->Action != PGOOptions::SampleUse)
-    LPM2.addPass(LoopFullUnrollPass(Level.getSpeedupLevel(),
-                                    /* OnlyWhenForced= */ !PTO.LoopUnrolling,
-                                    PTO.ForgetAllSCEVInLoopUnroll));
-
-  for (auto &C : LoopOptimizerEndEPCallbacks)
-    C(LPM2, Level);
-
-  // We provide the opt remark emitter pass for LICM to use. We only need to do
-  // this once as it is immutable.
-  FPM.addPass(
-      RequireAnalysisPass<OptimizationRemarkEmitterAnalysis, Function>());
-  FPM.addPass(createFunctionToLoopPassAdaptor(std::move(LPM1),
-                                              /*UseMemorySSA=*/true,
-                                              /*UseBlockFrequencyInfo=*/true));
-  FPM.addPass(
-      SimplifyCFGPass(SimplifyCFGOptions().convertSwitchRangeToICmp(true)));
-  FPM.addPass(InstCombinePass());
-  // The loop passes in LPM2 (LoopFullUnrollPass) do not preserve MemorySSA.
-  // *All* loop passes must preserve it, in order to be able to use it.
-  FPM.addPass(createFunctionToLoopPassAdaptor(std::move(LPM2),
-                                              /*UseMemorySSA=*/false,
-                                              /*UseBlockFrequencyInfo=*/false));
+  if (!SYCLOptimizationMode) {
+    // FIXME: re-association increases variables liveness and therefore register
+    // pressure.
+    FPM.addPass(ReassociatePass());
+
+    // Add the primary loop simplification pipeline.
+    // FIXME: Currently this is split into two loop pass pipelines because we run
+    // some function passes in between them. These can and should be removed
+    // and/or replaced by scheduling the loop pass equivalents in the correct
+    // positions. But those equivalent passes aren't powerful enough yet.
+    // Specifically, `SimplifyCFGPass` and `InstCombinePass` are currently still
+    // used. We have `LoopSimplifyCFGPass` which isn't yet powerful enough yet to
+    // fully replace `SimplifyCFGPass`, and the closest to the other we have is
+    // `LoopInstSimplify`.
+    LoopPassManager LPM1, LPM2;
+
+    // Simplify the loop body. We do this initially to clean up after other loop
+    // passes run, either when iterating on a loop or on inner loops with
+    // implications on the outer loop.
+    LPM1.addPass(LoopInstSimplifyPass());
+    LPM1.addPass(LoopSimplifyCFGPass());
+
+    // Try to remove as much code from the loop header as possible,
+    // to reduce amount of IR that will have to be duplicated. However,
+    // do not perform speculative hoisting the first time as LICM
+    // will destroy metadata that may not need to be destroyed if run
+    // after loop rotation.
+    // TODO: Investigate promotion cap for O1.
+    LPM1.addPass(LICMPass(PTO.LicmMssaOptCap, PTO.LicmMssaNoAccForPromotionCap,
+                          /*AllowSpeculation=*/false));
+
+    LPM1.addPass(LoopRotatePass(/* Disable header duplication */ true,
+                                isLTOPreLink(Phase)));
+    // TODO: Investigate promotion cap for O1.
+    LPM1.addPass(LICMPass(PTO.LicmMssaOptCap, PTO.LicmMssaNoAccForPromotionCap,
+                          /*AllowSpeculation=*/true));
+    LPM1.addPass(SimpleLoopUnswitchPass());
+    if (EnableLoopFlatten)
+      LPM1.addPass(LoopFlattenPass());
+
+    LPM2.addPass(LoopIdiomRecognizePass());
+    LPM2.addPass(IndVarSimplifyPass());
+
+    for (auto &C : LateLoopOptimizationsEPCallbacks)
+      C(LPM2, Level);
+
+    LPM2.addPass(LoopDeletionPass());
+
+    if (EnableLoopInterchange)
+      LPM2.addPass(LoopInterchangePass());
+
+    // Do not enable unrolling in PreLinkThinLTO phase during sample PGO
+    // because it changes IR to makes profile annotation in back compile
+    // inaccurate. The normal unroller doesn't pay attention to forced full unroll
+    // attributes so we need to make sure and allow the full unroll pass to pay
+    // attention to it.
+    if (Phase != ThinOrFullLTOPhase::ThinLTOPreLink || !PGOOpt ||
+        PGOOpt->Action != PGOOptions::SampleUse)
+      LPM2.addPass(LoopFullUnrollPass(Level.getSpeedupLevel(),
+                                      /* OnlyWhenForced= */ !PTO.LoopUnrolling,
+                                      PTO.ForgetAllSCEVInLoopUnroll));
+
+    for (auto &C : LoopOptimizerEndEPCallbacks)
+      C(LPM2, Level);
+
+    // We provide the opt remark emitter pass for LICM to use. We only need to do
+    // this once as it is immutable.
+    FPM.addPass(
+        RequireAnalysisPass<OptimizationRemarkEmitterAnalysis, Function>());
+    FPM.addPass(createFunctionToLoopPassAdaptor(std::move(LPM1),
+                                                /*UseMemorySSA=*/true,
+                                                /*UseBlockFrequencyInfo=*/true));
+    FPM.addPass(
+        SimplifyCFGPass(SimplifyCFGOptions().convertSwitchRangeToICmp(true)));
+    FPM.addPass(InstCombinePass());
+    // The loop passes in LPM2 (LoopFullUnrollPass) do not preserve MemorySSA.
+    // *All* loop passes must preserve it, in order to be able to use it.
+    FPM.addPass(createFunctionToLoopPassAdaptor(std::move(LPM2),
+                                                /*UseMemorySSA=*/false,
+                                                /*UseBlockFrequencyInfo=*/false));
+  }
 
   // Delete small array after loop unroll.
   FPM.addPass(SROAPass());
@@ -527,97 +535,104 @@ PassBuilder::buildFunctionSimplificationPipeline(OptimizationLevel Level,
   FPM.addPass(
       SimplifyCFGPass(SimplifyCFGOptions().convertSwitchRangeToICmp(true)));
 
+
   // Form canonically associated expression trees, and simplify the trees using
   // basic mathematical properties. For example, this will form (nearly)
   // minimal multiplication trees.
-  FPM.addPass(ReassociatePass());
-
-  // Add the primary loop simplification pipeline.
-  // FIXME: Currently this is split into two loop pass pipelines because we run
-  // some function passes in between them. These can and should be removed
-  // and/or replaced by scheduling the loop pass equivalents in the correct
-  // positions. But those equivalent passes aren't powerful enough yet.
-  // Specifically, `SimplifyCFGPass` and `InstCombinePass` are currently still
-  // used. We have `LoopSimplifyCFGPass` which isn't yet powerful enough yet to
-  // fully replace `SimplifyCFGPass`, and the closest to the other we have is
-  // `LoopInstSimplify`.
-  LoopPassManager LPM1, LPM2;
-
-  // Simplify the loop body. We do this initially to clean up after other loop
-  // passes run, either when iterating on a loop or on inner loops with
-  // implications on the outer loop.
-  LPM1.addPass(LoopInstSimplifyPass());
-  LPM1.addPass(LoopSimplifyCFGPass());
-
-  // Try to remove as much code from the loop header as possible,
-  // to reduce amount of IR that will have to be duplicated. However,
-  // do not perform speculative hoisting the first time as LICM
-  // will destroy metadata that may not need to be destroyed if run
-  // after loop rotation.
-  // TODO: Investigate promotion cap for O1.
-  LPM1.addPass(LICMPass(PTO.LicmMssaOptCap, PTO.LicmMssaNoAccForPromotionCap,
-                        /*AllowSpeculation=*/false));
-
-  // Disable header duplication in loop rotation at -Oz.
-  LPM1.addPass(
-      LoopRotatePass(Level != OptimizationLevel::Oz, isLTOPreLink(Phase)));
-  // TODO: Investigate promotion cap for O1.
-  LPM1.addPass(LICMPass(PTO.LicmMssaOptCap, PTO.LicmMssaNoAccForPromotionCap,
-                        /*AllowSpeculation=*/true));
-  LPM1.addPass(
-      SimpleLoopUnswitchPass(/* NonTrivial */ Level == OptimizationLevel::O3 &&
-                             EnableO3NonTrivialUnswitching));
-  if (EnableLoopFlatten)
-    LPM1.addPass(LoopFlattenPass());
-
-  LPM2.addPass(LoopIdiomRecognizePass());
-  LPM2.addPass(IndVarSimplifyPass());
-
-  for (auto &C : LateLoopOptimizationsEPCallbacks)
-    C(LPM2, Level);
-
-  LPM2.addPass(LoopDeletionPass());
-
-  if (EnableLoopInterchange)
-    LPM2.addPass(LoopInterchangePass());
-
-  // Do not enable unrolling in PreLinkThinLTO phase during sample PGO
-  // because it changes IR to makes profile annotation in back compile
-  // inaccurate. The normal unroller doesn't pay attention to forced full unroll
-  // attributes so we need to make sure and allow the full unroll pass to pay
-  // attention to it.
-  if (Phase != ThinOrFullLTOPhase::ThinLTOPreLink || !PGOOpt ||
-      PGOOpt->Action != PGOOptions::SampleUse)
-    LPM2.addPass(LoopFullUnrollPass(Level.getSpeedupLevel(),
-                                    /* OnlyWhenForced= */ !PTO.LoopUnrolling,
-                                    PTO.ForgetAllSCEVInLoopUnroll));
-
-  for (auto &C : LoopOptimizerEndEPCallbacks)
-    C(LPM2, Level);
-
-  // We provide the opt remark emitter pass for LICM to use. We only need to do
-  // this once as it is immutable.
-  FPM.addPass(
-      RequireAnalysisPass<OptimizationRemarkEmitterAnalysis, Function>());
-  FPM.addPass(createFunctionToLoopPassAdaptor(std::move(LPM1),
-                                              /*UseMemorySSA=*/true,
-                                              /*UseBlockFrequencyInfo=*/true));
-  FPM.addPass(
-      SimplifyCFGPass(SimplifyCFGOptions().convertSwitchRangeToICmp(true)));
-  FPM.addPass(InstCombinePass());
-  // The loop passes in LPM2 (LoopIdiomRecognizePass, IndVarSimplifyPass,
-  // LoopDeletionPass and LoopFullUnrollPass) do not preserve MemorySSA.
-  // *All* loop passes must preserve it, in order to be able to use it.
-  FPM.addPass(createFunctionToLoopPassAdaptor(std::move(LPM2),
-                                              /*UseMemorySSA=*/false,
-                                              /*UseBlockFrequencyInfo=*/false));
+  if (!SYCLOptimizationMode) {
+    // FIXME: re-association increases variables liveness and therefore register
+    // pressure.
+    FPM.addPass(ReassociatePass());
+
+    // Add the primary loop simplification pipeline.
+    // FIXME: Currently this is split into two loop pass pipelines because we run
+    // some function passes in between them. These can and should be removed
+    // and/or replaced by scheduling the loop pass equivalents in the correct
+    // positions. But those equivalent passes aren't powerful enough yet.
+    // Specifically, `SimplifyCFGPass` and `InstCombinePass` are currently still
+    // used. We have `LoopSimplifyCFGPass` which isn't yet powerful enough yet to
+    // fully replace `SimplifyCFGPass`, and the closest to the other we have is
+    // `LoopInstSimplify`.
+    LoopPassManager LPM1, LPM2;
+
+    // Simplify the loop body. We do this initially to clean up after other loop
+    // passes run, either when iterating on a loop or on inner loops with
+    // implications on the outer loop.
+    LPM1.addPass(LoopInstSimplifyPass());
+    LPM1.addPass(LoopSimplifyCFGPass());
+
+    // Try to remove as much code from the loop header as possible,
+    // to reduce amount of IR that will have to be duplicated. However,
+    // do not perform speculative hoisting the first time as LICM
+    // will destroy metadata that may not need to be destroyed if run
+    // after loop rotation.
+    // TODO: Investigate promotion cap for O1.
+    LPM1.addPass(LICMPass(PTO.LicmMssaOptCap, PTO.LicmMssaNoAccForPromotionCap,
+                          /*AllowSpeculation=*/false));
+
+    // Disable header duplication in loop rotation at -Oz.
+    LPM1.addPass(
+        LoopRotatePass(Level != OptimizationLevel::Oz, isLTOPreLink(Phase)));
+    // TODO: Investigate promotion cap for O1.
+    LPM1.addPass(LICMPass(PTO.LicmMssaOptCap, PTO.LicmMssaNoAccForPromotionCap,
+                          /*AllowSpeculation=*/true));
+    LPM1.addPass(
+        SimpleLoopUnswitchPass(/* NonTrivial */ Level == OptimizationLevel::O3 &&
+                               EnableO3NonTrivialUnswitching));
+    if (EnableLoopFlatten)
+      LPM1.addPass(LoopFlattenPass());
+
+    LPM2.addPass(LoopIdiomRecognizePass());
+    LPM2.addPass(IndVarSimplifyPass());
+
+    for (auto &C : LateLoopOptimizationsEPCallbacks)
+      C(LPM2, Level);
+
+    LPM2.addPass(LoopDeletionPass());
+
+    if (EnableLoopInterchange)
+      LPM2.addPass(LoopInterchangePass());
+
+    // Do not enable unrolling in PreLinkThinLTO phase during sample PGO
+    // because it changes IR to makes profile annotation in back compile
+    // inaccurate. The normal unroller doesn't pay attention to forced full unroll
+    // attributes so we need to make sure and allow the full unroll pass to pay
+    // attention to it.
+    if (Phase != ThinOrFullLTOPhase::ThinLTOPreLink || !PGOOpt ||
+        PGOOpt->Action != PGOOptions::SampleUse)
+      LPM2.addPass(LoopFullUnrollPass(Level.getSpeedupLevel(),
+                                      /* OnlyWhenForced= */ !PTO.LoopUnrolling,
+                                      PTO.ForgetAllSCEVInLoopUnroll));
+
+    for (auto &C : LoopOptimizerEndEPCallbacks)
+      C(LPM2, Level);
+
+    // We provide the opt remark emitter pass for LICM to use. We only need to do
+    // this once as it is immutable.
+    FPM.addPass(
+        RequireAnalysisPass<OptimizationRemarkEmitterAnalysis, Function>());
+    FPM.addPass(createFunctionToLoopPassAdaptor(std::move(LPM1),
+                                                /*UseMemorySSA=*/true,
+                                                /*UseBlockFrequencyInfo=*/true));
+    FPM.addPass(
+        SimplifyCFGPass(SimplifyCFGOptions().convertSwitchRangeToICmp(true)));
+    FPM.addPass(InstCombinePass());
+    // The loop passes in LPM2 (LoopIdiomRecognizePass, IndVarSimplifyPass,
+    // LoopDeletionPass and LoopFullUnrollPass) do not preserve MemorySSA.
+    // *All* loop passes must preserve it, in order to be able to use it.
+    FPM.addPass(createFunctionToLoopPassAdaptor(std::move(LPM2),
+                                                /*UseMemorySSA=*/false,
+                                                /*UseBlockFrequencyInfo=*/false));
+  }
 
   // Delete small array after loop unroll.
   FPM.addPass(SROAPass());
 
   // Try vectorization/scalarization transforms that are both improvements
   // themselves and can allow further folds with GVN and InstCombine.
-  FPM.addPass(VectorCombinePass(/*TryEarlyFoldsOnly=*/true));
+  // Disable for SYCL until SPIR-V reader is updated for all drivers.
+  if (!SYCLOptimizationMode)
+    FPM.addPass(VectorCombinePass(/*TryEarlyFoldsOnly=*/true));
 
   // Eliminate redundancies.
   FPM.addPass(MergedLoadStoreMotionPass());
@@ -668,10 +683,13 @@ PassBuilder::buildFunctionSimplificationPipeline(OptimizationLevel Level,
   for (auto &C : ScalarOptimizerLateEPCallbacks)
     C(FPM, Level);
 
-  FPM.addPass(SimplifyCFGPass(SimplifyCFGOptions()
-                                  .convertSwitchRangeToICmp(true)
-                                  .hoistCommonInsts(true)
-                                  .sinkCommonInsts(true)));
+  if (SYCLOptimizationMode)
+    FPM.addPass(SimplifyCFGPass());
+  else
+    FPM.addPass(SimplifyCFGPass(SimplifyCFGOptions()
+                                    .convertSwitchRangeToICmp(true)
+                                    .hoistCommonInsts(true)
+                                    .sinkCommonInsts(true)));
   FPM.addPass(InstCombinePass());
   invokePeepholeEPCallbacks(FPM, Level);
 
@@ -1304,29 +1322,31 @@ PassBuilder::buildModuleOptimizationPipeline(OptimizationLevel Level,
   for (auto &C : VectorizerStartEPCallbacks)
     C(OptimizePM, Level);
 
-  LoopPassManager LPM;
-  // First rotate loops that may have been un-rotated by prior passes.
-  // Disable header duplication at -Oz.
-  LPM.addPass(LoopRotatePass(Level != OptimizationLevel::Oz, LTOPreLink));
-  // Some loops may have become dead by now. Try to delete them.
-  // FIXME: see discussion in https://reviews.llvm.org/D112851,
-  //        this may need to be revisited once we run GVN before loop deletion
-  //        in the simplification pipeline.
-  LPM.addPass(LoopDeletionPass());
-  OptimizePM.addPass(createFunctionToLoopPassAdaptor(
-      std::move(LPM), /*UseMemorySSA=*/false, /*UseBlockFrequencyInfo=*/false));
-
-  // Distribute loops to allow partial vectorization.  I.e. isolate dependences
-  // into separate loop that would otherwise inhibit vectorization.  This is
-  // currently only performed for loops marked with the metadata
-  // llvm.loop.distribute=true or when -enable-loop-distribute is specified.
-  OptimizePM.addPass(LoopDistributePass());
-
-  // Populates the VFABI attribute with the scalar-to-vector mappings
-  // from the TargetLibraryInfo.
-  OptimizePM.addPass(InjectTLIMappings());
-
-  addVectorPasses(Level, OptimizePM, /* IsFullLTO */ false);
+  if (!SYCLOptimizationMode) {
+    LoopPassManager LPM;
+    // First rotate loops that may have been un-rotated by prior passes.
+    // Disable header duplication at -Oz.
+    LPM.addPass(LoopRotatePass(Level != OptimizationLevel::Oz, LTOPreLink));
+    // Some loops may have become dead by now. Try to delete them.
+    // FIXME: see discussion in https://reviews.llvm.org/D112851,
+    //        this may need to be revisited once we run GVN before loop deletion
+    //        in the simplification pipeline.
+    LPM.addPass(LoopDeletionPass());
+    OptimizePM.addPass(createFunctionToLoopPassAdaptor(
+        std::move(LPM), /*UseMemorySSA=*/false, /*UseBlockFrequencyInfo=*/false));
+
+    // Distribute loops to allow partial vectorization.  I.e. isolate dependences
+    // into separate loop that would otherwise inhibit vectorization.  This is
+    // currently only performed for loops marked with the metadata
+    // llvm.loop.distribute=true or when -enable-loop-distribute is specified.
+    OptimizePM.addPass(LoopDistributePass());
+
+    // Populates the VFABI attribute with the scalar-to-vector mappings
+    // from the TargetLibraryInfo.
+    OptimizePM.addPass(InjectTLIMappings());
+
+    addVectorPasses(Level, OptimizePM, /* IsFullLTO */ false);
+  }
 
   // LoopSink pass sinks instructions hoisted by LICM, which serves as a
   // canonicalization pass that enables other optimizations. As a result,
diff --git a/llvm/lib/SYCLLowerIR/CMakeLists.txt b/llvm/lib/SYCLLowerIR/CMakeLists.txt
new file mode 100644
index 000000000000..1bd3e9a8c35d
--- /dev/null
+++ b/llvm/lib/SYCLLowerIR/CMakeLists.txt
@@ -0,0 +1,34 @@
+if(${CMAKE_VERSION} VERSION_LESS 3.14)
+    macro(FetchContent_MakeAvailable NAME)
+        FetchContent_GetProperties(${NAME})
+        if(NOT ${NAME}_POPULATED)
+            FetchContent_Populate(${NAME})
+            add_subdirectory(${${NAME}_SOURCE_DIR} ${${NAME}_BINARY_DIR})
+        endif()
+    endmacro()
+endif()
+
+add_llvm_component_library(LLVMSYCLLowerIR
+  LowerWGScope.cpp
+  LowerWGLocalMemory.cpp
+  MutatePrintfAddrspace.cpp
+
+  LocalAccessorToSharedMemory.cpp
+  TargetHelpers.cpp
+
+  ADDITIONAL_HEADER_DIRS
+  ${LLVM_MAIN_INCLUDE_DIR}/llvm/SYCLLowerIR
+
+  DEPENDS
+  LLVMDemangle
+  LLVMTransformUtils
+
+  LINK_LIBS
+  LLVMDemangle
+  LLVMTransformUtils
+  
+  LINK_COMPONENTS
+  Analysis
+  Core
+  Support
+  )
diff --git a/llvm/lib/SYCLLowerIR/LLVMBuild.txt b/llvm/lib/SYCLLowerIR/LLVMBuild.txt
new file mode 100644
index 000000000000..fb5071556e1f
--- /dev/null
+++ b/llvm/lib/SYCLLowerIR/LLVMBuild.txt
@@ -0,0 +1,21 @@
+;===- ./lib/SYCLLowerIR/LLVMBuild.txt -----------------------------*- Conf -*--===;
+;
+; Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+; See https://llvm.org/LICENSE.txt for license information.
+; SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+;
+;===------------------------------------------------------------------------===;
+;
+; This is an LLVMBuild description file for the components in this subdirectory.
+;
+; For more information on the LLVMBuild system, please see:
+;
+;   http://llvm.org/docs/LLVMBuild.html
+;
+;===------------------------------------------------------------------------===;
+
+[component_0]
+type = Library
+name = SYCLLowerIR
+parent = Libraries
+required_libraries = Core Support
diff --git a/llvm/lib/SYCLLowerIR/LocalAccessorToSharedMemory.cpp b/llvm/lib/SYCLLowerIR/LocalAccessorToSharedMemory.cpp
new file mode 100644
index 000000000000..8dbf5176806d
--- /dev/null
+++ b/llvm/lib/SYCLLowerIR/LocalAccessorToSharedMemory.cpp
@@ -0,0 +1,214 @@
+//===- LocalAccessorToSharedMemory.cpp - Local Accessor Support for CUDA --===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/SYCLLowerIR/LocalAccessorToSharedMemory.h"
+#include "llvm/IR/Constants.h"
+#include "llvm/IR/GlobalValue.h"
+#include "llvm/IR/Instructions.h"
+#include "llvm/IR/PassManager.h"
+#include "llvm/Pass.h"
+#include "llvm/Support/CommandLine.h"
+#include "llvm/Transforms/IPO.h"
+
+using namespace llvm;
+
+#define DEBUG_TYPE "localaccessortosharedmemory"
+
+// Legacy PM wrapper.
+namespace {
+class LocalAccessorToSharedMemoryLegacy : public ModulePass {
+public:
+  static char ID;
+
+  LocalAccessorToSharedMemoryLegacy() : ModulePass(ID) {
+    initializeLocalAccessorToSharedMemoryLegacyPass(
+        *PassRegistry::getPassRegistry());
+  }
+
+  bool runOnModule(Module &M) override {
+    ModuleAnalysisManager MAM;
+    auto PA = Impl.run(M, MAM);
+    return !PA.areAllPreserved();
+  }
+
+private:
+  LocalAccessorToSharedMemoryPass Impl;
+};
+} // namespace
+
+char LocalAccessorToSharedMemoryLegacy::ID = 0;
+INITIALIZE_PASS(LocalAccessorToSharedMemoryLegacy,
+                "localaccessortosharedmemory",
+                "SYCL Local Accessor to Shared Memory", false, false)
+
+ModulePass *llvm::createLocalAccessorToSharedMemoryPassLegacy() {
+  return new LocalAccessorToSharedMemoryLegacy();
+}
+
+// New PM implementation.
+PreservedAnalyses
+LocalAccessorToSharedMemoryPass::run(Module &M, ModuleAnalysisManager &) {
+  const auto AT = TargetHelpers::getArchType(M);
+
+  // Invariant: This pass is only intended to operate on SYCL kernels being
+  // compiled to either `nvptx{,64}-nvidia-cuda`, or `amdgcn-amd-amdhsa`
+  // triples.
+  if (ArchType::Unsupported == AT)
+    return PreservedAnalyses::all();
+
+  SmallVector<KernelPayload, 4> Kernels;
+  TargetHelpers::populateKernels(M, Kernels, AT);
+  SmallVector<std::pair<Function *, KernelPayload>, 4> NewToOldKernels;
+  if (Kernels.empty())
+    return PreservedAnalyses::all();
+
+  // Process the function and if changed, update the metadata.
+  for (auto K : Kernels) {
+    auto *NewKernel = processKernel(M, K.Kernel);
+    if (NewKernel)
+      NewToOldKernels.push_back(std::make_pair(NewKernel, K));
+  }
+
+  if (NewToOldKernels.empty())
+    return PreservedAnalyses::all();
+
+  postProcessKernels(NewToOldKernels);
+
+  return PreservedAnalyses::none();
+}
+
+Function *LocalAccessorToSharedMemoryPass::processKernel(Module &M,
+                                                         Function *F) {
+  // Check if this function is eligible by having an argument that uses shared
+  // memory.
+  const bool UsesLocalMemory =
+      std::any_of(F->arg_begin(), F->arg_end(), [&](Argument &FA) {
+        return FA.getType()->isPointerTy() &&
+               FA.getType()->getPointerAddressSpace() == SharedASValue;
+      });
+
+  // Skip functions which are not eligible.
+  if (!UsesLocalMemory)
+    return nullptr;
+
+  // Create a global symbol to CUDA's ADDRESS_SPACE_SHARED or AMD's
+  // LOCAL_ADDRESS.
+  auto SharedMemGlobalName = F->getName().str();
+  SharedMemGlobalName.append("_shared_mem");
+  auto *SharedMemGlobalType =
+      ArrayType::get(Type::getInt8Ty(M.getContext()), 0);
+  auto *SharedMemGlobal = new GlobalVariable(
+      /* Module= */ M,
+      /* Type= */ &*SharedMemGlobalType,
+      /* IsConstant= */ false,
+      /* Linkage= */ GlobalValue::ExternalLinkage,
+      /* Initializer= */ nullptr,
+      /* Name= */ Twine{SharedMemGlobalName},
+      /* InsertBefore= */ nullptr,
+      /* ThreadLocalMode= */ GlobalValue::NotThreadLocal,
+      /* AddressSpace= */ SharedASValue,
+      /* IsExternallyInitialized= */ false);
+  SharedMemGlobal->setAlignment(Align(4));
+
+  FunctionType *FTy = F->getFunctionType();
+  const AttributeList &FAttrs = F->getAttributes();
+
+  // Store the arguments and attributes for the new function, as well as which
+  // arguments were replaced.
+  std::vector<Type *> Arguments;
+  SmallVector<AttributeSet, 8> ArgumentAttributes;
+  SmallVector<bool, 10> ArgumentReplaced(FTy->getNumParams(), false);
+
+  for (const auto &I : enumerate(F->args())) {
+    const Argument &FA = I.value();
+    if (FA.getType()->isPointerTy() &&
+        FA.getType()->getPointerAddressSpace() == SharedASValue) {
+      // Replace pointers to shared memory with i32 offsets.
+      Arguments.push_back(Type::getInt32Ty(M.getContext()));
+      ArgumentAttributes.push_back(
+          AttributeSet::get(M.getContext(), ArrayRef<Attribute>{}));
+      ArgumentReplaced[I.index()] = true;
+    } else {
+      // Replace other arguments with the same type as before.
+      Arguments.push_back(FA.getType());
+      ArgumentAttributes.push_back(FAttrs.getParamAttrs(I.index()));
+    }
+  }
+
+  // Create new function type.
+  AttributeList NAttrs =
+      AttributeList::get(F->getContext(), FAttrs.getFnAttrs(),
+                         FAttrs.getRetAttrs(), ArgumentAttributes);
+  FunctionType *NFTy =
+      FunctionType::get(FTy->getReturnType(), Arguments, FTy->isVarArg());
+
+  // Create the new function body and insert it into the module.
+  Function *NF = Function::Create(NFTy, F->getLinkage(), F->getAddressSpace(),
+                                  Twine{""}, &M);
+  NF->copyAttributesFrom(F);
+  NF->setComdat(F->getComdat());
+  NF->setAttributes(NAttrs);
+  NF->takeName(F);
+
+  // Splice the body of the old function right into the new function.
+  NF->getBasicBlockList().splice(NF->begin(), F->getBasicBlockList());
+
+  unsigned i = 0;
+  for (Function::arg_iterator FA = F->arg_begin(), FE = F->arg_end(),
+                              NFA = NF->arg_begin();
+       FA != FE; ++FA, ++NFA, ++i) {
+    Value *NewValueForUse = NFA;
+    if (ArgumentReplaced[i]) {
+      // If this argument was replaced, then create a `getelementptr`
+      // instruction that uses it to recreate the pointer that was replaced.
+      auto *InsertBefore = &NF->getEntryBlock().front();
+      auto *PtrInst = GetElementPtrInst::CreateInBounds(
+          /* PointeeType= */ SharedMemGlobalType,
+          /* Ptr= */ SharedMemGlobal,
+          /* IdxList= */
+          ArrayRef<Value *>{
+              ConstantInt::get(Type::getInt32Ty(M.getContext()), 0, false),
+              NFA,
+          },
+          /* NameStr= */ Twine{NFA->getName()}, InsertBefore);
+      // Then create a bitcast to make sure the new pointer is the same type
+      // as the old one. This will only ever be a `i8 addrspace(3)*` to `i32
+      // addrspace(3)*` type of cast.
+      auto *CastInst = new BitCastInst(PtrInst, FA->getType());
+      CastInst->insertAfter(PtrInst);
+      NewValueForUse = CastInst;
+    }
+
+    // Replace uses of the old function's argument with the new argument or
+    // the result of the `getelementptr`/`bitcast` instructions.
+    FA->replaceAllUsesWith(&*NewValueForUse);
+    NewValueForUse->takeName(&*FA);
+  }
+
+  // There should be no callers of kernel entry points.
+  assert(F->use_empty());
+
+  // Clone metadata of the old function, including debug info descriptor.
+  SmallVector<std::pair<unsigned, MDNode *>, 1> MDs;
+  F->getAllMetadata(MDs);
+  for (auto MD : MDs)
+    NF->addMetadata(MD.first, *MD.second);
+
+  // Now that the old function is dead, delete it.
+  F->eraseFromParent();
+
+  return NF;
+}
+
+void LocalAccessorToSharedMemoryPass::postProcessKernels(
+    SmallVectorImpl<std::pair<Function *, KernelPayload>> &NewToOldKernels) {
+  for (auto &Pair : NewToOldKernels) {
+    std::get<1>(Pair).MD->replaceOperandWith(
+        0, llvm::ConstantAsMetadata::get(std::get<0>(Pair)));
+  }
+}
diff --git a/llvm/lib/SYCLLowerIR/LowerWGLocalMemory.cpp b/llvm/lib/SYCLLowerIR/LowerWGLocalMemory.cpp
new file mode 100644
index 000000000000..1ca82ae078df
--- /dev/null
+++ b/llvm/lib/SYCLLowerIR/LowerWGLocalMemory.cpp
@@ -0,0 +1,124 @@
+//===-- LowerWGLocalMemory.cpp - SYCL kernel local memory allocation pass -===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+// See intro comments in the header.
+//===----------------------------------------------------------------------===//
+
+#include "llvm/SYCLLowerIR/LowerWGLocalMemory.h"
+#include "llvm/IR/Function.h"
+#include "llvm/IR/IRBuilder.h"
+#include "llvm/IR/InstIterator.h"
+#include "llvm/Pass.h"
+
+using namespace llvm;
+
+#define DEBUG_TYPE "LowerWGLocalMemory"
+
+static constexpr char SYCL_ALLOCLOCALMEM_CALL[] = "__sycl_allocateLocalMemory";
+static constexpr char LOCALMEMORY_GV_PREF[] = "WGLocalMem";
+
+namespace {
+class SYCLLowerWGLocalMemoryLegacy : public ModulePass {
+public:
+  static char ID;
+
+  SYCLLowerWGLocalMemoryLegacy() : ModulePass(ID) {
+    initializeSYCLLowerWGLocalMemoryLegacyPass(
+        *PassRegistry::getPassRegistry());
+  }
+
+  bool runOnModule(Module &M) override {
+    ModuleAnalysisManager DummyMAM;
+    auto PA = Impl.run(M, DummyMAM);
+    return !PA.areAllPreserved();
+  }
+
+private:
+  SYCLLowerWGLocalMemoryPass Impl;
+};
+} // namespace
+
+char SYCLLowerWGLocalMemoryLegacy::ID = 0;
+INITIALIZE_PASS(SYCLLowerWGLocalMemoryLegacy, "sycllowerwglocalmemory",
+                "Replace __sycl_allocateLocalMemory with allocation of memory "
+                "in local address space",
+                false, false)
+
+ModulePass *llvm::createSYCLLowerWGLocalMemoryLegacyPass() {
+  return new SYCLLowerWGLocalMemoryLegacy();
+}
+
+// TODO: It should be checked that __sycl_allocateLocalMemory (or its source
+// form - group_local_memory) does not occur:
+//  - in a function (other than user lambda/functor)
+//  - in a loop
+//  - in a non-convergent control flow
+// to make it consistent with OpenCL restriction.
+// But LLVM pass is not the best place to diagnose these cases.
+// Error checking should be done in the front-end compiler.
+static void lowerAllocaLocalMemCall(CallInst *CI, Module &M) {
+  assert(CI);
+
+  Value *ArgSize = CI->getArgOperand(0);
+  uint64_t Size = cast<llvm::ConstantInt>(ArgSize)->getZExtValue();
+  Value *ArgAlign = CI->getArgOperand(1);
+  uint64_t Alignment = cast<llvm::ConstantInt>(ArgAlign)->getZExtValue();
+
+  IRBuilder<> Builder(CI);
+  Type *LocalMemArrayTy = ArrayType::get(Builder.getInt8Ty(), Size);
+  unsigned LocalAS =
+      CI->getFunctionType()->getReturnType()->getPointerAddressSpace();
+  auto *LocalMemArrayGV =
+      new GlobalVariable(M,                                // module
+                         LocalMemArrayTy,                  // type
+                         false,                            // isConstant
+                         GlobalValue::InternalLinkage,     // Linkage
+                         UndefValue::get(LocalMemArrayTy), // Initializer
+                         LOCALMEMORY_GV_PREF,              // Name prefix
+                         nullptr,                          // InsertBefore
+                         GlobalVariable::NotThreadLocal,   // ThreadLocalMode
+                         LocalAS                           // AddressSpace
+      );
+  LocalMemArrayGV->setAlignment(Align(Alignment));
+
+  Value *GVPtr =
+      Builder.CreatePointerCast(LocalMemArrayGV, Builder.getInt8PtrTy(LocalAS));
+  CI->replaceAllUsesWith(GVPtr);
+}
+
+static bool allocaWGLocalMemory(Module &M) {
+  Function *ALMFunc = M.getFunction(SYCL_ALLOCLOCALMEM_CALL);
+  if (!ALMFunc)
+    return false;
+
+  assert(ALMFunc->isDeclaration() && "should have declaration only");
+
+  SmallVector<CallInst *, 4> DelCalls;
+  for (User *U : ALMFunc->users()) {
+    auto *CI = cast<CallInst>(U);
+    lowerAllocaLocalMemCall(CI, M);
+    DelCalls.push_back(CI);
+  }
+
+  for (auto *CI : DelCalls) {
+    assert(CI->use_empty() && "removing live instruction");
+    CI->eraseFromParent();
+  }
+
+  // Remove __sycl_allocateLocalMemory declaration.
+  assert(ALMFunc->use_empty() && "__sycl_allocateLocalMemory is still in use");
+  ALMFunc->eraseFromParent();
+
+  return true;
+}
+
+PreservedAnalyses SYCLLowerWGLocalMemoryPass::run(Module &M,
+                                                  ModuleAnalysisManager &) {
+  if (allocaWGLocalMemory(M))
+    return PreservedAnalyses::none();
+  return PreservedAnalyses::all();
+}
diff --git a/llvm/lib/SYCLLowerIR/LowerWGScope.cpp b/llvm/lib/SYCLLowerIR/LowerWGScope.cpp
new file mode 100644
index 000000000000..f1ad112c331e
--- /dev/null
+++ b/llvm/lib/SYCLLowerIR/LowerWGScope.cpp
@@ -0,0 +1,984 @@
+//===-- LowerWGScope.cpp - lower work group scope code and locals ---------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+// Simple work group (WG) code and data lowering pass. SYCL specification
+// requires that
+// - the code in the parallel_for_work_group (PFWG) but outside the
+//    parallel_for_work_item (PFWI) (called "work group scope" further here) is
+//    executed once per WG
+// - data declared at the work group scope is shared among work items (WIs) in
+//    a WG.
+// - private_memory<T> data declared at the work group scope remains private per
+//   physical work item and lives across all parallel_for_work_item invocations
+//
+// To enforce this semantics, this pass
+// - inserts "if (get_local_id(0)) == 0" guards ("is leader" guard) to disable
+//   WG-scope code execution in "worker" WIs
+// - transforms allocas in the PFWG lambda function; the function is identified
+//   by the "work_group_scope" string metadata added by the Front End
+//
+// There are 3 kinds of local variables in the PFWG lambda function which are
+// handled differently by the compiler:
+// 1) Local variables of type private_memory<T> declared by the user. FE marks
+//    allocas created for them with "work_item_scope" string metadata.
+// 2) Other local variables declared by the user (shared). Front end turns them
+//    into globals in the local address space - work group shared locals. There
+//    are no allocas for them in the PFWG lambda.
+// 3) Compiler-generated locals:
+//    - the PFWI lambda object (1 per PFWI) which captures variables passed into
+//      the PFWI lambda
+//    - a local copy of the PFWG lambda object parameter passed by value into
+//      the PFWG lambda
+//
+// ** Kind 2: no further transformations are needed for kind 2.
+// ** Kind 3:
+// For a kind 3 variable (alloca w/o metadata) this pass creates a WG-shared
+// local "shadow" variable. Before each PFWI invocation leader WI stores its
+// private copy of the variable into the shadow (under "is leader" guard), then
+// all WIs (outside of "is leader" guard) load the shadow value into their
+// private copies ("materialize" the private copy). This works because these
+// variables are uniform - i.e. have the same value in all WIs and are not
+// changed within PFWI. The only exceptions are captures of private_memory
+// instances - see next.
+// ** Kind 1:
+// Even though WG-scope locals are supposed to be uniform, there is one
+// exception - capture of local of kind 1. It is always captured by non-const
+// reference because as there no
+// 'const T &operator()(const h_item<Dimensions> &id);' which means the result
+// of kind 1 variable's alloca is stored within the PFWI lambda.
+// Materialization of the lambda object value writes result of alloca of the
+// leader WI's private variable into the private copy of the lambda object,
+// which is wrong. So for these variables this pass adds a write of the private
+// variable's address into the private copy of the lambda object right after its
+// materialization:
+//     if (is_leader())
+//       *PFWI_lambda_obj_shadow_addr = *PFWI_lambda_obj_alloca;
+//     barrier();
+// (1) *PFWI_lambda_obj_alloca = *PFWI_lambda_obj_shadow_addr;
+// (2) PFWI_lambda_obj_alloca->priv_var_addr = priv_var_alloca;
+//     parallel_for_work_item(..., PFWI_lambda_obj_alloca);
+//
+// (1) - materialization of a PFWI object
+// (2) - "fixup" of the private variable address.
+//
+// TODO: add support for the case when there are other functions between
+// parallel_for_work_group and parallel_for_work_item in the call stack.
+// For example:
+//
+// void foo(sycl::group<1> group, ...) {
+//   group.parallel_for_work_item(range<1>(), [&](h_item<1> i) { ... });
+// }
+// ...
+//   cgh.parallel_for_work_group<class kernel>(
+//     range<1>(...), range<1>(...), [=](group<1> g) {
+//       foo(g, ...);
+//     });
+//
+// TODO The approach employed by this pass generates lots of barriers and data
+// copying between private and local memory, which might not be efficient. There
+// are optimization opportunities listed below. Also other approaches can be
+// considered like
+// "Efficient Fork-Join on GPUs through Warp Specialization" by Arpith C. Jacob
+// et. al.
+//===----------------------------------------------------------------------===//
+
+#include "llvm/SYCLLowerIR/LowerWGScope.h"
+#include "llvm/ADT/SmallPtrSet.h"
+#include "llvm/ADT/SmallVector.h"
+#include "llvm/ADT/Statistic.h"
+#include "llvm/ADT/Triple.h"
+#include "llvm/IR/IRBuilder.h"
+#include "llvm/IR/Instructions.h"
+#include "llvm/IR/IntrinsicInst.h"
+#include "llvm/IR/Module.h"
+#include "llvm/InitializePasses.h"
+#include "llvm/Pass.h"
+#include "llvm/Support/CommandLine.h"
+
+#ifndef NDEBUG
+#include "llvm/IR/CFG.h"
+#include "llvm/IR/Verifier.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/GraphWriter.h"
+#endif
+
+using namespace llvm;
+
+#define DEBUG_TYPE "lowerwgcode"
+
+STATISTIC(LocalMemUsed, "amount of additional local memory used for sharing");
+
+static constexpr char WG_SCOPE_MD[] = "work_group_scope";
+static constexpr char WI_SCOPE_MD[] = "work_item_scope";
+static constexpr char PFWI_MD[] = "parallel_for_work_item";
+
+static cl::opt<int> Debug("sycl-lower-wg-debug", llvm::cl::Optional,
+                          llvm::cl::Hidden,
+                          llvm::cl::desc("Debug SYCL work group code lowering"),
+                          llvm::cl::init(1));
+
+namespace {
+class SYCLLowerWGScopeLegacyPass : public FunctionPass {
+public:
+  static char ID; // Pass identification, replacement for typeid
+  SYCLLowerWGScopeLegacyPass() : FunctionPass(ID) {
+    initializeSYCLLowerWGScopeLegacyPassPass(*PassRegistry::getPassRegistry());
+  }
+
+  // run the LowerWGScope pass on the specified module
+  bool runOnFunction(Function &F) override {
+    FunctionAnalysisManager FAM;
+    auto PA = Impl.run(F, FAM);
+    return !PA.areAllPreserved();
+  }
+
+private:
+  SYCLLowerWGScopePass Impl;
+};
+} // namespace
+
+char SYCLLowerWGScopeLegacyPass::ID = 0;
+INITIALIZE_PASS(SYCLLowerWGScopeLegacyPass, "LowerWGScope",
+                "Lower Work Group Scope Code", false, false)
+
+// Public interface to the SYCLLowerWGScopePass.
+FunctionPass *llvm::createSYCLLowerWGScopePass() {
+  return new SYCLLowerWGScopeLegacyPass();
+}
+
+template <typename T> static unsigned asUInt(T val) {
+  return static_cast<unsigned>(val);
+}
+
+static IntegerType *getSizeTTy(Module &M) {
+  LLVMContext &Ctx = M.getContext();
+  auto PtrSize = M.getDataLayout().getPointerTypeSize(Type::getInt8PtrTy(Ctx));
+  return PtrSize == 8 ? Type::getInt64Ty(Ctx) : Type::getInt32Ty(Ctx);
+}
+
+// Encapsulates SPIR-V-dependent code generation.
+// TODO this should be factored out into a separate project in clang
+namespace spirv {
+// There is no TargetMachine for SPIR-V, so define those inline here
+enum class AddrSpace : unsigned {
+  Private = 0,
+  Global = 1,
+  Constant = 2,
+  Local = 3,
+  Generic = 4,
+  Input = 5,
+  Output = 6
+};
+
+enum class Scope : unsigned {
+  CrossDevice = 0,
+  Device = 1,
+  Workgroup = 2,
+  Subgroup = 3,
+  Invocation = 4,
+};
+
+enum class MemorySemantics : unsigned {
+  None = 0x0,
+  Acquire = 0x2,
+  Release = 0x4,
+  AcquireRelease = 0x8,
+  SequentiallyConsistent = 0x10,
+  UniformMemory = 0x40,
+  SubgroupMemory = 0x80,
+  WorkgroupMemory = 0x100,
+  CrossWorkgroupMemory = 0x200,
+  AtomicCounterMemory = 0x400,
+  ImageMemory = 0x800,
+};
+
+Instruction *genWGBarrier(Instruction &Before, const Triple &TT);
+Value *genPseudoLocalID(Instruction &Before, const Triple &TT);
+GlobalVariable *createWGLocalVariable(Module &M, Type *T, const Twine &Name);
+} // namespace spirv
+
+static bool isCallToAFuncMarkedWithMD(const Instruction *I, const char *MD) {
+  const CallInst *Call = dyn_cast<CallInst>(I);
+  const Function *F =
+      dyn_cast_or_null<Function>(Call ? Call->getCalledFunction() : nullptr);
+  return F && F->getMetadata(MD);
+}
+
+// Checks is this is a call to parallel_for_work_item.
+static bool isPFWICall(const Instruction *I) {
+  return isCallToAFuncMarkedWithMD(I, PFWI_MD);
+}
+
+// Checks if given instruction must be executed by all work items.
+static bool isWIScopeInst(const Instruction *I) {
+  if (I->isTerminator())
+    return true;
+
+  switch (I->getOpcode()) {
+  case Instruction::Alloca: {
+    llvm_unreachable("allocas must have been skipped");
+    return true;
+  }
+  case Instruction::PHI:
+    llvm_unreachable("PHIs must have been skipped");
+    return true;
+  case Instruction::Call:
+    return isCallToAFuncMarkedWithMD(I, WI_SCOPE_MD);
+  default:
+    return false;
+  }
+}
+
+// Checks if given instruction may have side effects visible outside current
+// work item.
+static bool mayHaveSideEffects(const Instruction *I) {
+  if (I->isTerminator())
+    return false;
+
+  switch (I->getOpcode()) {
+  case Instruction::Alloca:
+    llvm_unreachable("allocas must have been handled");
+    return false;
+  case Instruction::PHI:
+    llvm_unreachable("PHIs must have been skipped");
+    return false;
+  case Instruction::Call:
+    assert(!isPFWICall(I) && "pfwi must have been handled separately");
+    return true;
+  case Instruction::AddrSpaceCast:
+    return false;
+  default:
+    return true;
+  }
+}
+
+// Generates control flow which disables execution of TrueBB in worker WIs:
+//   IfBB:
+//     ...
+//     %a = load i64, i64 addrspace(1)* @__spirv_BuiltInLocalInvocationIndex
+//     %b = icmp eq i64 %a, 0
+//     br i1 %b, label %TrueBB, label %MergeBB
+//
+//   TrueBB:
+//     ...
+//     br label %MergeBB
+//
+//   MergeBB:
+//     ...
+// IfBB's terminator instruction is replaced with the branch.
+//
+static void guardBlockWithIsLeaderCheck(BasicBlock *IfBB, BasicBlock *TrueBB,
+                                        BasicBlock *MergeBB,
+                                        const DebugLoc &DbgLoc,
+                                        const Triple &TT) {
+  Value *LinearLocalID = spirv::genPseudoLocalID(*IfBB->getTerminator(), TT);
+  auto *Ty = LinearLocalID->getType();
+  Value *Zero = Constant::getNullValue(Ty);
+  IRBuilder<> Builder(IfBB->getContext());
+  spirv::genWGBarrier(*(IfBB->getTerminator()), TT);
+  Builder.SetInsertPoint(IfBB->getTerminator());
+  Value *Cmp = Builder.CreateICmpEQ(LinearLocalID, Zero, "cmpz");
+  Builder.SetCurrentDebugLocation(DbgLoc);
+  Builder.CreateCondBr(Cmp, TrueBB, MergeBB);
+  IfBB->getTerminator()->eraseFromParent();
+  assert(TrueBB->getSingleSuccessor() == MergeBB && "CFG tform error");
+}
+
+static void
+shareOutputViaLocalMem(Instruction &I, BasicBlock &BBa, BasicBlock &BBb,
+                       SmallPtrSetImpl<Instruction *> &LeaderScope) {
+
+  SmallPtrSet<Instruction *, 4> Users;
+
+  for (auto User : I.users()) {
+    Instruction *UI = dyn_cast<Instruction>(User);
+    if (!UI || LeaderScope.find(UI) != LeaderScope.end())
+      // not interested in the user if it is within the scope
+      continue;
+    Users.insert(UI);
+  }
+  // Skip instruction w/o uses or if all its uses lie within the scope
+  if (Users.size() == 0)
+    return;
+  LLVMContext &Ctx = I.getContext();
+  Type *T = I.getType();
+  // 1) Create WG local variable
+  Value *WGLocal = spirv::createWGLocalVariable(*I.getModule(), T,
+                                                I.getFunction()->getName() +
+                                                    "WG_" + Twine(I.getName()));
+  // 2) Generate a store of the produced value into the WG local var
+  IRBuilder<> Bld(Ctx);
+  Bld.SetInsertPoint(I.getNextNode());
+  Bld.CreateStore(&I, WGLocal);
+  // 3) Generate a load in the "worker" BB of the value stored by the leader
+  Bld.SetInsertPoint(&BBb.front());
+  auto *WGVal = Bld.CreateLoad(T, WGLocal, "wg_val_" + Twine(I.getName()));
+  // 4) Finally, replace usages of I outside the scope
+  for (auto *U : Users)
+    U->replaceUsesOfWith(&I, WGVal);
+}
+
+using InstrRange = std::pair<Instruction *, Instruction *>;
+
+// Input IR, where I1..IN is the range. I1 has uses outside the range:
+//   A
+//   %I1 = ...;
+//   ... USE1(%I1) ...
+//   %IN = ...;
+//   B
+//   ... USE2(%I1) ...
+//
+// Resulting basic blocks:
+// BBa:
+//   A
+//   %linear_id = call get_linear_local_id()
+//   %is_leader = cmp %linear_id, 0
+//   branch %is_leader LeaderBB, BB
+//
+// LeaderBB:
+//   %I1 = ...;
+//   store %I1, @WG_I1
+//   ... USE1(%I1) ...
+//   %IN = ...;
+//   store %IN, @WG_I1
+//   branch BBb
+//
+// BBb:
+//   call WG_control_barrier()
+//   %I1_new = load @WG_I1
+//   ...
+//   B
+//   ... USE2(%I1_new) ...
+static void tformRange(const InstrRange &R, const Triple &TT) {
+  // Instructions seen between the first and the last
+  SmallPtrSet<Instruction *, 16> Seen;
+  Instruction *FirstSE = R.first;
+  Instruction *LastSE = R.second;
+  LLVM_DEBUG(llvm::dbgs() << "Tform range {\n  " << *FirstSE << "\n  "
+                          << *LastSE << "\n}\n");
+  assert(FirstSE->getParent() == LastSE->getParent() && "invalid range");
+
+  for (auto *I = FirstSE; I != LastSE; I = I->getNextNode())
+    Seen.insert(I);
+  Seen.insert(LastSE);
+
+  BasicBlock *BBa = FirstSE->getParent();
+  BasicBlock *LeaderBB = BBa->splitBasicBlock(FirstSE, "wg_leader");
+  BasicBlock *BBb = LeaderBB->splitBasicBlock(LastSE->getNextNode(), "wg_cf");
+
+  // 1) insert the first "is work group leader" test (at the first split) for
+  //     the worker WIs to detour the side effects instructions
+  guardBlockWithIsLeaderCheck(BBa, LeaderBB, BBb, FirstSE->getDebugLoc(), TT);
+
+  // 2) "Share" the output values of the instructions in the range
+  for (auto *I : Seen)
+    shareOutputViaLocalMem(*I, *BBa, *BBb, Seen);
+
+  // 3) Insert work group barrier so that workers further read valid data
+  //    (before the materialization reads inserted at step 2)
+  spirv::genWGBarrier(BBb->front(), TT);
+}
+
+namespace {
+using LocalsSet = SmallPtrSet<AllocaInst *, 4>;
+}
+
+static void copyBetweenPrivateAndShadow(Value *L, GlobalVariable *Shadow,
+                                        IRBuilder<> &Builder, bool Loc2Shadow) {
+  assert(isa<PointerType>(L->getType()));
+  Type *T = nullptr;
+  MaybeAlign LocAlign(0);
+
+  if (const auto *AI = dyn_cast<AllocaInst>(L)) {
+    T = AI->getAllocatedType();
+    LocAlign = AI->getAlign();
+  } else {
+    auto Arg = cast<Argument>(L);
+    T = Arg->getParamByValType();
+    LocAlign = Arg->getParamAlign();
+  }
+
+  assert(T && "Unexpected type");
+
+  if (T->isAggregateType()) {
+    // TODO: we should use methods which directly return MaybeAlign once such
+    // are added to LLVM for AllocaInst and GlobalVariable
+    auto ShdAlign = MaybeAlign(Shadow->getAlignment());
+    Module &M = *Shadow->getParent();
+    auto SizeVal = M.getDataLayout().getTypeStoreSize(T);
+    auto Size = ConstantInt::get(getSizeTTy(M), SizeVal);
+    if (Loc2Shadow)
+      Builder.CreateMemCpy(Shadow, ShdAlign, L, LocAlign, Size);
+    else
+      Builder.CreateMemCpy(L, LocAlign, Shadow, ShdAlign, Size);
+  } else {
+    Value *Src = L;
+    Value *Dst = Shadow;
+
+    if (!Loc2Shadow)
+      std::swap(Src, Dst);
+    Value *LocalVal = Builder.CreateLoad(T, Src, "mat_ld");
+    Builder.CreateStore(LocalVal, Dst);
+  }
+}
+
+// Performs the following transformation for each basic block in the input map:
+//
+// BB:
+//   some_instructions
+// =>
+// TestBB:
+//   %linear_id = call get_linear_local_id()
+//   %is_leader = cmp %linear_id, 0
+//   branch %is_leader LeaderBB, OriginalBB
+//
+// LeaderBB:
+//   *@Shadow_local1 = *local1
+//   ...
+//   *@Shadow_localN = *localN
+//   branch BB
+//
+// BB:
+//   call WG_control_barrier()
+//   *local1 = *@Shadow_local1
+//   ...
+//   *localN = *@Shadow_localN
+//   some_instructions
+//
+// Where:
+// - local<i> is the set of to-be-materialized locals for OriginalBB taken from
+//   the first input map
+// - @Shadow_local<i> is the shadow workgroup-shared global variable for
+// local<i>,
+//   taken from the second input map
+//
+static void materializeLocalsInWIScopeBlocksImpl(
+    const DenseMap<BasicBlock *, std::unique_ptr<LocalsSet>> &BB2MatLocals,
+    const DenseMap<AllocaInst *, GlobalVariable *> &Local2Shadow,
+    const Triple &TT) {
+  for (auto &P : BB2MatLocals) {
+    // generate LeaderBB and private<->shadow copies in proper BBs
+    BasicBlock *LeaderBB = P.first;
+    BasicBlock *BB = LeaderBB->splitBasicBlock(&LeaderBB->front(), "LeaderMat");
+    // Add a barrier to the original block:
+    Instruction *At =
+        spirv::genWGBarrier(*BB->getFirstNonPHI(), TT)->getNextNode();
+
+    for (AllocaInst *L : *P.second.get()) {
+      auto MapEntry = Local2Shadow.find(L);
+      assert(MapEntry != Local2Shadow.end() && "local must have a shadow");
+      auto *Shadow = MapEntry->second;
+      LLVMContext &Ctx = L->getContext();
+      IRBuilder<> Builder(Ctx);
+      // fill the leader BB:
+      // fetch data from leader's private copy (which is always up to date) into
+      // the corresponding shadow variable
+      Builder.SetInsertPoint(&LeaderBB->front());
+      copyBetweenPrivateAndShadow(L, Shadow, Builder, true /*private->shadow*/);
+      // store data to the local variable - effectively "refresh" the value of
+      // the local in each work item in the work group
+      Builder.SetInsertPoint(At);
+      copyBetweenPrivateAndShadow(L, Shadow, Builder,
+                                  false /*shadow->private*/);
+    }
+    // now generate the TestBB and the leader WI guard
+    BasicBlock *TestBB =
+        LeaderBB->splitBasicBlock(&LeaderBB->front(), "TestMat");
+    std::swap(TestBB, LeaderBB);
+    guardBlockWithIsLeaderCheck(TestBB, LeaderBB, BB, At->getDebugLoc(), TT);
+  }
+}
+
+// Checks if there is a need to materialize value of given local in given work
+// item-scope basic block.
+static bool localMustBeMaterialized(const AllocaInst *L, const BasicBlock &BB) {
+  // TODO this is overly conservative - see speculations below.
+  return true;
+}
+
+// This function handles locals of kind 3 (see comments at the top of file).
+//
+// For each alloca the following transformation is done for each WI scope basic
+// block basic_block10 where the alloca is used:
+//
+//   T *p = alloca(T);
+//   if (is_leader) { use1(p); } // WG scope
+//   ...
+// basic_block10: // WI scope basic block (executed by all WIs)
+//   use2(p);
+// =>
+//   T *p = alloca(T);
+//   if (is_leader) { use1(p); }
+//   ...
+// // p materialization code; note that all locals in the WG scope are uniform
+//   if (is_leader) { *@Shadow_p = *p; } // store actual value of the local
+//   barrier(); // make sure workers wait till the value write above is complete
+//   branch basic_block10;
+// basic_block10: // WI scope basic block
+//   *p = *@Shadow_p; // materialize the value in the local variable before use;
+//                 // maybe skipped for the leader, but does not seem worth it
+//                 // as the leader WI is just a vector lane, so there should be
+//                 // one load per thread (subgroup) anyway.
+//   use2(p);
+//
+// NOTE:
+// Simply redirecting all the p uses (dereferences) in WI scope blocks is not
+// enough in the general case. E.g. consider this example:
+//
+//   T *p = alloca(T);
+//   T *p1 = p+10;
+//   if (is_leader) { use1(p); } // WG scope
+//   ...
+// basic_block10: // WI scope
+//   use2(p1);
+//
+// TODO. This implementation is quite ineffective. Currently it materializes
+// all locals in all WI scope basic blocks.
+// Will be improved incrementally:
+// - For each alloca: determine all derived (via GEPs) pointers and make sure
+//   they don't escape. Then check if there are reads in current WI scope BB
+//   through either of those. If none of them escape and there are no reads then
+//   materialization of this alloca in this BB is not needed.
+// - Materialization is not needed if there is dominating BB with materialized
+//   value, and there are no WG scope writes to this alloca on any path from
+//   that BB to current.
+// - Avoid unnecessary '*p = *@Shadow_p' reloads and redirect p uses them to the
+//   @Shadow_p in case it can be proved it is safe (see note above). Might not
+//   have any noticeable effect, though, as reading from Shadow always goes to a
+//   register file anyway.
+//
+void materializeLocalsInWIScopeBlocks(SmallPtrSetImpl<AllocaInst *> &Locals,
+                                      SmallPtrSetImpl<BasicBlock *> &WIScopeBBs,
+                                      const Triple &TT) {
+  // maps local variable to its "shadow" workgroup-shared global:
+  DenseMap<AllocaInst *, GlobalVariable *> Local2Shadow;
+  // records which locals must be materialized at the beginning of a block:
+  DenseMap<BasicBlock *, std::unique_ptr<LocalsSet>> BB2MatLocals;
+
+  // TODO: iterating over BBs first then over locals would require less
+  // book-keeping with current implementation, but later improvements will need
+  // global info like mapping BBs to locals sets to optimize.
+
+  // Fill the local-to-shadow and basic block-to-locals maps:
+  for (auto L : Locals) {
+    for (auto *BB : WIScopeBBs) {
+      if (!localMustBeMaterialized(L, *BB))
+        continue;
+      if (Local2Shadow.find(L) == Local2Shadow.end()) {
+        // lazily create a "shadow" for current local:
+        GlobalVariable *Shadow = spirv::createWGLocalVariable(
+            *BB->getModule(), L->getAllocatedType(), "WGCopy");
+        Local2Shadow.insert(std::make_pair(L, Shadow));
+      }
+      auto &MatLocals = BB2MatLocals[BB];
+
+      if (!MatLocals.get()) {
+        // lazily create a locals set for current BB:
+        MatLocals.reset(new LocalsSet());
+      }
+      MatLocals->insert(L);
+    }
+  }
+  // perform the materialization
+  materializeLocalsInWIScopeBlocksImpl(BB2MatLocals, Local2Shadow, TT);
+}
+
+#ifndef NDEBUG
+static void dumpDot(const Function &F, const Twine &Suff) {
+  std::error_code EC;
+  auto FName =
+      ("PFWG_Kernel_" + Suff + "_" + Twine(F.getValueID()) + ".dot").str();
+  raw_fd_ostream File(FName, EC, sys::fs::OF_Text);
+
+  if (!EC)
+    WriteGraph(File, (const Function *)&F, false);
+  else
+    errs() << "  error opening file for writing: << " << FName << "\n";
+}
+
+static void dumpIR(const Function &F, const Twine &Suff) {
+  std::error_code EC;
+  auto FName =
+      ("PFWG_Kernel_" + Suff + "_" + Twine(F.getValueID()) + ".ll").str();
+  raw_fd_ostream File(FName, EC, sys::fs::OF_Text);
+
+  if (!EC)
+    F.print(File, 0, 1, 1);
+  else
+    errs() << "  error opening file for writing: << " << FName << "\n";
+}
+#endif // NDEBUG
+
+using CaptureDesc = std::pair<AllocaInst *, GetElementPtrInst *>;
+
+// This function handles locals of kind 1 (see comments at the top of file) -
+// captures of private_memory<T> variables. It basically adds (*) instruction in
+// the pattern below.
+//     if (is_leader())
+//       *PFWI_lambda_obj_shadow_addr = *PFWI_lambda_obj_alloca;
+//     barrier();
+//     *PFWI_lambda_obj_alloca = *PFWI_lambda_obj_shadow_addr;
+// (*) PFWI_lambda_obj_alloca->priv_var_addr = priv_var_alloca;
+//     parallel_for_work_item(..., PFWI_lambda_obj_alloca);
+//
+static void fixupPrivateMemoryPFWILambdaCaptures(CallInst *PFWICall) {
+  // Lambda object is always the last argument to the PFWI lambda function:
+  auto NArgs = PFWICall->arg_size();
+  if (PFWICall->arg_size() == 1)
+    return;
+
+  Value *LambdaObj =
+      PFWICall->getArgOperand(NArgs - 1 /*lambda object parameter*/);
+  // First go through all stores through the LambdaObj pointer - those are
+  // initialization of captures, and for each stored value find its origin -
+  // whether it is an alloca with "work_item_scope"
+  SmallVector<CaptureDesc, 4> PrivMemCaptures;
+
+  // Look through cast
+  if (auto *Cast = dyn_cast<AddrSpaceCastInst>(LambdaObj))
+    LambdaObj = Cast->getOperand(0);
+
+  for (auto *U : LambdaObj->users()) {
+    GetElementPtrInst *GEP = dyn_cast<GetElementPtrInst>(U);
+
+    if (!GEP)
+      continue;
+    assert(GEP->hasOneUse());
+    StoreInst *CaptureInit = dyn_cast<StoreInst>(*(GEP->users().begin()));
+
+    if (!CaptureInit)
+      // this can't be private_memory<T> capture, which is always captured by
+      // address via the StoreInst instruction
+      continue;
+    Value *StoredVal = CaptureInit->getValueOperand();
+    // '[=]' capture of a private_memory<T> instance is not permitted, there is
+    // no 'const T &operator()(const h_item<Dimensions> &id);', so compiler
+    // would generate an error; this means captured value is always a pointer -
+    // whether it is private_memory instance or some other type
+    if (!StoredVal->getType()->isPointerTy())
+      continue;
+
+    while (StoredVal && !isa<AllocaInst>(StoredVal)) {
+      if (auto *BC = dyn_cast<BitCastInst>(StoredVal)) {
+        StoredVal = BC->getOperand(0);
+        continue;
+      }
+      if (auto *ASC = dyn_cast<AddrSpaceCastInst>(StoredVal)) {
+        StoredVal = ASC->getOperand(0);
+        continue;
+      }
+      StoredVal = nullptr; // something else is captured
+      break;
+    }
+    auto *AI = dyn_cast_or_null<AllocaInst>(StoredVal);
+
+    // only private_memory allocations (allocas marked with "work_item_scope"
+    // are of interest here:
+    if (!AI || !AI->getMetadata(WI_SCOPE_MD))
+      continue;
+    PrivMemCaptures.push_back(CaptureDesc{AI, GEP});
+  }
+  // now rewrite the captured address of a private_memory variables within the
+  // PFWI lambda object:
+  for (auto &C : PrivMemCaptures) {
+    GetElementPtrInst *NewGEP = cast<GetElementPtrInst>(C.second->clone());
+    NewGEP->insertBefore(PFWICall);
+    IRBuilder<> Bld(PFWICall->getContext());
+    Bld.SetInsertPoint(PFWICall);
+    Value *Val = C.first;
+    auto ValAS = cast<PointerType>(Val->getType())->getAddressSpace();
+    auto PtrAS =
+        cast<PointerType>(NewGEP->getResultElementType())->getAddressSpace();
+
+    if (ValAS != PtrAS)
+      Val = Bld.CreateAddrSpaceCast(Val, NewGEP->getResultElementType());
+    Bld.CreateStore(Val, NewGEP);
+  }
+}
+
+// Go through "byval" parameters which are passed as AS(0) pointers
+// and: (1) create local shadows for them (2) and initialize them from the
+// leader's copy and (3) materialize the value in the local variable before use
+static void shareByValParams(Function &F, const Triple &TT) {
+  // Skip alloca instructions and split. Alloca instructions must be in the
+  // beginning of the function otherwise they are considered as dynamic which
+  // can cause the problems with inlining.
+  BasicBlock *EntryBB = &F.getEntryBlock();
+  Instruction *SplitPoint = &*EntryBB->begin();
+  for (; SplitPoint->getOpcode() == Instruction::Alloca;
+       SplitPoint = SplitPoint->getNextNode())
+    ;
+  BasicBlock *LeaderBB = EntryBB->splitBasicBlock(SplitPoint, "leader");
+  BasicBlock *MergeBB = LeaderBB->splitBasicBlock(&LeaderBB->front(), "merge");
+
+  // Rewire the above basic blocks so that LeaderBB is executed only for the
+  // leader workitem
+  guardBlockWithIsLeaderCheck(EntryBB, LeaderBB, MergeBB,
+                              EntryBB->back().getDebugLoc(), TT);
+  Instruction &At = LeaderBB->back();
+
+  for (auto &Arg : F.args()) {
+    if (!Arg.hasByValAttr())
+      continue;
+
+    assert(Arg.getType()->getPointerAddressSpace() ==
+           asUInt(spirv::AddrSpace::Private));
+
+    // Create the shared copy - "shadow" - for current arg
+    Type *T = Arg.getParamByValType();
+    GlobalVariable *Shadow =
+        spirv::createWGLocalVariable(*F.getParent(), T, "ArgShadow");
+
+    LLVMContext &Ctx = At.getContext();
+    IRBuilder<> Builder(Ctx);
+    Builder.SetInsertPoint(&LeaderBB->front());
+
+    copyBetweenPrivateAndShadow(&Arg, Shadow, Builder,
+                                true /*private->shadow*/);
+    // Materialize the value in the local variable before use
+    Builder.SetInsertPoint(&MergeBB->front());
+    copyBetweenPrivateAndShadow(&Arg, Shadow, Builder,
+                                false /*shadow->private*/);
+  }
+  // Insert barrier to make sure workers use up-to-date shared values written by
+  // the leader
+  spirv::genWGBarrier(MergeBB->front(), TT);
+}
+
+PreservedAnalyses SYCLLowerWGScopePass::run(Function &F,
+                                            FunctionAnalysisManager &FAM) {
+  if (!F.getMetadata(WG_SCOPE_MD))
+    return PreservedAnalyses::all();
+  LLVM_DEBUG(llvm::dbgs() << "Function name: " << F.getName() << "\n");
+  const auto &TT = llvm::Triple(F.getParent()->getTargetTriple());
+  // Ranges of "side effect" instructions
+  SmallVector<InstrRange, 16> Ranges;
+  SmallPtrSet<AllocaInst *, 16> Allocas;
+  SmallPtrSet<Instruction *, 16> WIScopeInsts;
+  SmallPtrSet<CallInst *, 4> PFWICalls;
+
+  // Collect the ranges which need transformation
+  for (auto &BB : F) {
+    // first and last instructions with side effects, which must be executed
+    // only once per work group:
+    Instruction *First = nullptr;
+    Instruction *Last = nullptr;
+
+    // Skip PHIs, allocas and addrspacecasts associated with allocas, as they
+    // don't have side effects and must never be guarded with the WG leader
+    // test. Note that there should be no allocas in local address space at this
+    // point - they must have been converted to globals.
+    Instruction *I = BB.getFirstNonPHI();
+
+    for (; I->getOpcode() == Instruction::Alloca ||
+           I->getOpcode() == Instruction::AddrSpaceCast ||
+           I->isDebugOrPseudoInst();
+         I = I->getNextNode()) {
+      auto *AllocaI = dyn_cast<AllocaInst>(I);
+      // Allocas marked with "work_item_scope" are those originating from
+      // sycl::private_memory<T> variables, which must be in private memory.
+      // No shadows/materialization is needed for them because they can be
+      // updated only within PFWIs
+      if (AllocaI && !AllocaI->getMetadata(WI_SCOPE_MD))
+        Allocas.insert(AllocaI);
+    }
+    for (; I && (I != BB.getTerminator()); I = I->getNextNode()) {
+      if (isWIScopeInst(I)) {
+        if (isPFWICall(I))
+          PFWICalls.insert(dyn_cast<CallInst>(I));
+        WIScopeInsts.insert(I);
+        LLVM_DEBUG(llvm::dbgs() << "+++ Exec by all: " << *I << "\n");
+        // need to split the range here, because the instruction must be
+        // executed by all work items - force range addition
+        if (First) {
+          assert(Last && "range must have been closed 1");
+          Ranges.push_back(InstrRange{First, Last});
+          First = nullptr;
+          Last = nullptr;
+        }
+        continue;
+      }
+      if (!mayHaveSideEffects(I))
+        continue;
+      LLVM_DEBUG(llvm::dbgs() << "+++ Side effects: " << *I << "\n");
+      if (!First)
+        First = I;
+      Last = I;
+    }
+    if (First) {
+      assert(Last && "range must have been closed 2");
+      Ranges.push_back(InstrRange{First, Last});
+    }
+  }
+
+  int NByval = 0;
+  for (const auto &Arg : F.args()) {
+    if (Arg.hasByValAttr())
+      NByval++;
+  }
+
+  bool HaveChanges = (Ranges.size() > 0) || (Allocas.size() > 0) || NByval > 0;
+
+#ifndef NDEBUG
+  if (HaveChanges && Debug > 1) {
+    dumpIR(F, "before");
+    dumpDot(F, "before");
+  }
+#endif // NDEBUG
+
+  // Perform the transformation
+  for (auto &R : Ranges)
+    tformRange(R, TT);
+
+  // There can be allocas not corresponding to any variable declared in user
+  // code but generated by the compiler - e.g. for non-trivially typed
+  // parameters passed by value. There can be WG scope stores into such
+  // allocas, which need to be made visible to all WIs. This is done via
+  // creating a "shadow" workgroup-shared variable and using it to propagate
+  // the value of the alloca'ed variable to worker WIs from the leader.
+
+  // First collect WIScope BBs where locals will be materialized:
+  SmallPtrSet<BasicBlock *, 16> WIScopeBBs;
+
+  for (auto *I : WIScopeInsts)
+    WIScopeBBs.insert(I->getParent());
+
+  // Now materialize the locals:
+  materializeLocalsInWIScopeBlocks(Allocas, WIScopeBBs, TT);
+
+  // Fixup captured addresses of private_memory instances in current WI
+  for (auto *PFWICall : PFWICalls)
+    fixupPrivateMemoryPFWILambdaCaptures(PFWICall);
+
+  // Finally, create shadows for and replace usages of byval pointer params.
+  shareByValParams(F, TT);
+
+#ifndef NDEBUG
+  if (HaveChanges && Debug > 0)
+    verifyModule(*F.getParent(), &llvm::errs());
+  if (HaveChanges && Debug > 1) {
+    dumpIR(F, "after");
+    dumpDot(F, "after");
+  }
+#endif // NDEBUG
+  return HaveChanges ? PreservedAnalyses::none() : PreservedAnalyses::all();
+}
+
+GlobalVariable *spirv::createWGLocalVariable(Module &M, Type *T,
+                                             const Twine &Name) {
+  GlobalVariable *G =
+      new GlobalVariable(M,                              // module
+                         T,                              // type
+                         false,                          // isConstant
+                         GlobalValue::InternalLinkage,   // Linkage
+                         UndefValue::get(T),             // Initializer
+                         Name,                           // Name
+                         nullptr,                        // InsertBefore
+                         GlobalVariable::NotThreadLocal, // ThreadLocalMode
+                         asUInt(spirv::AddrSpace::Local) // AddressSpace
+      );
+  G->setUnnamedAddr(GlobalValue::UnnamedAddr::Global);
+  const DataLayout &DL = M.getDataLayout();
+  G->setAlignment(MaybeAlign(DL.getPreferredAlign(G)));
+  LocalMemUsed += DL.getTypeStoreSize(G->getValueType());
+  LLVM_DEBUG(llvm::dbgs() << "Local AS Var created: " << G->getName() << "\n");
+  LLVM_DEBUG(llvm::dbgs() << "  Local mem used: " << LocalMemUsed << "B\n");
+  return G;
+}
+
+// Functions below expose SPIR-V translator-specific intrinsics to the use
+// in LLVM IR. Those calls and global references will be translated to
+// corresponding SPIR-V operations and builtin variables.
+//
+// TODO generalize to support all SPIR-V intrinsic operations and builtin
+//      variables
+
+// Return a value equals to 0 if and only if the local linear id is 0.
+Value *spirv::genPseudoLocalID(Instruction &Before, const Triple &TT) {
+  Module &M = *Before.getModule();
+  if (TT.isNVPTX() || TT.isAMDGCN()) {
+    LLVMContext &Ctx = Before.getContext();
+    Type *RetTy = getSizeTTy(M);
+
+    IRBuilder<> Bld(Ctx);
+    Bld.SetInsertPoint(&Before);
+
+#define CREATE_CALLEE(NAME, FN_NAME)                                           \
+  FunctionCallee FnCallee##NAME = M.getOrInsertFunction(FN_NAME, RetTy);       \
+  assert(FnCallee##NAME && "spirv intrinsic creation failed");                 \
+  auto NAME = Bld.CreateCall(FnCallee##NAME, {});
+
+    CREATE_CALLEE(LocalInvocationId_X, "_Z27__spirv_LocalInvocationId_xv");
+    CREATE_CALLEE(LocalInvocationId_Y, "_Z27__spirv_LocalInvocationId_yv");
+    CREATE_CALLEE(LocalInvocationId_Z, "_Z27__spirv_LocalInvocationId_zv");
+
+#undef CREATE_CALLEE
+
+    // 1: returns
+    //   __spirv_LocalInvocationId_x() |
+    //   __spirv_LocalInvocationId_y() |
+    //   __spirv_LocalInvocationId_z()
+    //
+    return Bld.CreateOr(LocalInvocationId_X,
+                        Bld.CreateOr(LocalInvocationId_Y, LocalInvocationId_Z));
+  } else {
+    // extern "C" const __constant size_t __spirv_BuiltInLocalInvocationIndex;
+    // Must correspond to the code in
+    // llvm-spirv/lib/SPIRV/OCL20ToSPIRV.cpp
+    // OCL20ToSPIRV::transWorkItemBuiltinsToVariables()
+    StringRef Name = "__spirv_BuiltInLocalInvocationIndex";
+    GlobalVariable *G = M.getGlobalVariable(Name);
+
+    if (!G) {
+      Type *T = getSizeTTy(M);
+      G = new GlobalVariable(M,                              // module
+                             T,                              // type
+                             true,                           // isConstant
+                             GlobalValue::ExternalLinkage,   // Linkage
+                             nullptr,                        // Initializer
+                             Name,                           // Name
+                             nullptr,                        // InsertBefore
+                             GlobalVariable::NotThreadLocal, // ThreadLocalMode
+                             // TODO 'Input' crashes CPU Back-End
+                             // asUInt(spirv::AddrSpace::Input) // AddressSpace
+                             asUInt(spirv::AddrSpace::Global) // AddressSpace
+      );
+      Align Alignment = M.getDataLayout().getPreferredAlign(G);
+      G->setAlignment(MaybeAlign(Alignment));
+    }
+    Value *Res = new LoadInst(G->getValueType(), G, "", &Before);
+    return Res;
+  }
+}
+
+// extern void __spirv_ControlBarrier(Scope Execution, Scope Memory,
+//  uint32_t Semantics) noexcept;
+Instruction *spirv::genWGBarrier(Instruction &Before, const Triple &TT) {
+  Module &M = *Before.getModule();
+  StringRef Name = "_Z22__spirv_ControlBarrierjjj";
+  LLVMContext &Ctx = Before.getContext();
+  Type *ScopeTy = Type::getInt32Ty(Ctx);
+  Type *SemanticsTy = Type::getInt32Ty(Ctx);
+  Type *RetTy = Type::getVoidTy(Ctx);
+
+  AttributeList Attr;
+  Attr = Attr.addFnAttribute(Ctx, Attribute::Convergent);
+  FunctionCallee FC =
+      M.getOrInsertFunction(Name, Attr, RetTy, ScopeTy, ScopeTy, SemanticsTy);
+  assert(FC.getCallee() && "spirv intrinsic creation failed");
+
+  IRBuilder<> Bld(Ctx);
+  Bld.SetInsertPoint(&Before);
+  auto ArgExec = ConstantInt::get(ScopeTy, asUInt(spirv::Scope::Workgroup));
+  auto ArgMem = ConstantInt::get(ScopeTy, asUInt(spirv::Scope::Workgroup));
+  auto ArgSema = ConstantInt::get(
+      ScopeTy, asUInt(spirv::MemorySemantics::SequentiallyConsistent) |
+                   asUInt(spirv::MemorySemantics::WorkgroupMemory));
+  auto BarrierCall = Bld.CreateCall(FC, {ArgExec, ArgMem, ArgSema});
+  BarrierCall->addFnAttr(llvm::Attribute::Convergent);
+  return BarrierCall;
+}
diff --git a/llvm/lib/SYCLLowerIR/README.txt b/llvm/lib/SYCLLowerIR/README.txt
new file mode 100644
index 000000000000..123180a4d119
--- /dev/null
+++ b/llvm/lib/SYCLLowerIR/README.txt
@@ -0,0 +1,3 @@
+A collection of SYCL language-specific LLVM IR transformation passes applied to
+the result of clang code generation. They bring the IR to a valid state from
+the SYCL semantics perspective.
diff --git a/llvm/lib/SYCLLowerIR/TargetHelpers.cpp b/llvm/lib/SYCLLowerIR/TargetHelpers.cpp
new file mode 100644
index 000000000000..a4a7e35cfc29
--- /dev/null
+++ b/llvm/lib/SYCLLowerIR/TargetHelpers.cpp
@@ -0,0 +1,88 @@
+//===----------- TargetHelpers.cpp - Helpers for SYCL kernels ------------ ===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// Helper functions for processing SYCL kernels.
+//
+//===----------------------------------------------------------------------===//
+
+#include "llvm/SYCLLowerIR/TargetHelpers.h"
+#include "llvm/ADT/SmallSet.h"
+#include "llvm/ADT/StringSwitch.h"
+#include "llvm/IR/Metadata.h"
+
+using namespace llvm;
+
+namespace llvm {
+namespace TargetHelpers {
+
+KernelPayload::KernelPayload(Function *Kernel, MDNode *MD)
+    : Kernel(Kernel), MD(MD) {}
+
+ArchType getArchType(const Module &M) {
+  return StringSwitch<ArchType>(M.getTargetTriple().c_str())
+      .Case("nvptx64-nvidia-cuda", ArchType::Cuda)
+      .Case("nvptx-nvidia-cuda", ArchType::Cuda)
+      .Case("amdgcn-amd-amdhsa", ArchType::AMDHSA)
+      .Case("amdgcn--amdhsa", ArchType::AMDHSA)
+      .Default(ArchType::Unsupported);
+}
+
+std::string getAnnotationString(ArchType AT) {
+  switch (AT) {
+  case TargetHelpers::ArchType::Cuda:
+    return std::string("nvvm.annotations");
+    break;
+  case TargetHelpers::ArchType::AMDHSA:
+    return std::string("amdgcn.annotations");
+    break;
+  default:
+    llvm_unreachable("Unsupported arch type.");
+  }
+  return std::string();
+}
+
+void populateKernels(Module &M, SmallVectorImpl<KernelPayload> &Kernels,
+                     ArchType AT) {
+  // Access `{amdgcn|nvvm}.annotations` to determine which functions are kernel
+  // entry points.
+  std::string Annotation = getAnnotationString(AT);
+  auto *AnnotationMetadata = M.getNamedMetadata(Annotation);
+  // No kernels in the module, early exit.
+  if (!AnnotationMetadata)
+    return;
+
+  // It is possible that the annotations node contains multiple pointers to the
+  // same metadata, recognise visited ones.
+  SmallSet<MDNode *, 4> Visited;
+  for (auto *MetadataNode : AnnotationMetadata->operands()) {
+    if (Visited.contains(MetadataNode) || MetadataNode->getNumOperands() != 3)
+      continue;
+
+    Visited.insert(MetadataNode);
+
+    // Kernel entry points are identified using metadata nodes of the form:
+    //   !X = !{<function>, !"kernel", i32 1}
+    auto *Type = dyn_cast<MDString>(MetadataNode->getOperand(1));
+    if (!Type)
+      continue;
+    // Only process kernel entry points.
+    if (Type->getString() != "kernel")
+      continue;
+
+    // Get a pointer to the entry point function from the metadata.
+    const MDOperand &FuncOperand = MetadataNode->getOperand(0);
+    if (!FuncOperand)
+      continue;
+    if (auto *FuncConstant = dyn_cast<ConstantAsMetadata>(FuncOperand))
+      if (auto *Func = dyn_cast<Function>(FuncConstant->getValue()))
+        Kernels.push_back(KernelPayload(Func, MetadataNode));
+  }
+}
+
+} // namespace TargetHelpers
+} // namespace llvm
